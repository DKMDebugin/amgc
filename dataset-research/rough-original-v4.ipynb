{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbookretina/Library/Python/3.7/lib/python/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n",
      "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n",
      "  from numba.decorators import jit as optional_jit\n",
      "/Users/macbookretina/Library/Python/3.7/lib/python/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n",
      "  from numba.decorators import jit as optional_jit\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "from pandas.api.types import CategoricalDtype\n",
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pywt\n",
    "from scipy.stats import skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MOUNTED_DATASET_PATH = '/Users/macbookretina/Desktop/mount-s3-bucket'\n",
    "SAMPLE_FILE = '/Users/macbookretina/blues.00042.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(filepath):\n",
    "    \"\"\"\n",
    "    This method was adapted from the FMA: A Dataset For Music Analysis.\n",
    "    \"\"\"\n",
    "    filename = os.path.basename(filepath)\n",
    "\n",
    "    if 'features' in filename:\n",
    "        return pd.read_csv(filepath, index_col=0, header=[0, 1, 2])\n",
    "\n",
    "    if 'echonest' in filename:\n",
    "        return pd.read_csv(filepath, index_col=0, header=[0, 1, 2])\n",
    "\n",
    "    if 'genres' in filename:\n",
    "        return pd.read_csv(filepath, index_col=None)\n",
    "\n",
    "    if 'tracks' in filename:\n",
    "        tracks = pd.read_csv(filepath, index_col=0, header=[0, 1])\n",
    "\n",
    "        COLUMNS = [('track', 'tags'), ('album', 'tags'), ('artist', 'tags'),\n",
    "                   ('track', 'genres'), ('track', 'genres_all')]\n",
    "        for column in COLUMNS:\n",
    "            tracks[column] = tracks[column].map(ast.literal_eval)\n",
    "\n",
    "        COLUMNS = [('track', 'date_created'), ('track', 'date_recorded'),\n",
    "                   ('album', 'date_created'), ('album', 'date_released'),\n",
    "                   ('artist', 'date_created'), ('artist', 'active_year_begin'),\n",
    "                   ('artist', 'active_year_end')]\n",
    "        for column in COLUMNS:\n",
    "            tracks[column] = pd.to_datetime(tracks[column])\n",
    "\n",
    "        SUBSETS = ('small', 'medium', 'large')\n",
    "        tracks['set', 'subset'] = tracks['set', 'subset'].astype(\n",
    "                CategoricalDtype(categories=SUBSETS, ordered=True))\n",
    "\n",
    "        COLUMNS = [('track', 'genre_top'), ('track', 'license'),\n",
    "                   ('album', 'type'), ('album', 'information'),\n",
    "                   ('artist', 'bio')]\n",
    "        for column in COLUMNS:\n",
    "            tracks[column] = tracks[column].astype('category')\n",
    "\n",
    "        return tracks\n",
    "        \n",
    "def stats(feature):\n",
    "    return {\n",
    "        'mean': np.mean(feature), \n",
    "        'median': np.median(feature), \n",
    "        'std': np.std(feature), \n",
    "        'var': np.var(feature)\n",
    "    }\n",
    "\n",
    "def extra_stats(feature):\n",
    "    return {\n",
    "        'sb_energy': np.mean(np.abs(feature)),\n",
    "        'skewness': skew(feature)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store for all features to be extracted except log-mel and mel-spectogram.\n",
    "dataframe = pd.DataFrame({\n",
    "    'genre_label': [],\n",
    "    'data_source': [],\n",
    "    \n",
    "    'mean_spec_centroid': [],\n",
    "    'median_spec_centroid': [],\n",
    "    'std_spec_centroid': [],\n",
    "    'var_spec_centroid': [],\n",
    "    \n",
    "    'mean_spec_rolloff': [],\n",
    "    'median_spec_rolloff': [],\n",
    "    'std_spec_rolloff': [],\n",
    "    'var_spec_rolloff': [],\n",
    "    \n",
    "    'mean_zcr': [],\n",
    "    'median_zcr': [],\n",
    "    'std_zcr': [],\n",
    "    'var_zcr': [],\n",
    "    \n",
    "    'mean_spec_bw': [],\n",
    "    'median_spec_bw': [],\n",
    "    'std_spec_bw': [],\n",
    "    'var_spec_bw': [],\n",
    "    \n",
    "    'mean_spec_contrast_1': [],\n",
    "    'median_spec_contrast_1': [],\n",
    "    'std_spec_contrast_1': [],\n",
    "    'var_spec_contrast_1': [],\n",
    "    \n",
    "    'mean_spec_contrast_2': [],\n",
    "    'median_spec_contrast_2': [],\n",
    "    'std_spec_contrast_2': [],\n",
    "    'var_spec_contrast_2': [],\n",
    "    \n",
    "    'mean_spec_contrast_3': [],\n",
    "    'median_spec_contrast_3': [],\n",
    "    'std_spec_contrast_3': [],\n",
    "    'var_spec_contrast_3': [],\n",
    "    \n",
    "    'mean_spec_contrast_4': [],\n",
    "    'median_spec_contrast_4': [],\n",
    "    'std_spec_contrast_4': [],\n",
    "    'var_spec_contrast_4': [],\n",
    "    \n",
    "    'mean_spec_contrast_5': [],\n",
    "    'median_spec_contrast_5': [],\n",
    "    'std_spec_contrast_5': [],\n",
    "    'var_spec_contrast_5': [],\n",
    "    \n",
    "    'mean_spec_contrast_6': [],\n",
    "    'median_spec_contrast_6': [],\n",
    "    'std_spec_contrast_6': [],\n",
    "    'var_spec_contrast_6': [],\n",
    "    \n",
    "    'mean_spec_contrast_7': [],\n",
    "    'median_spec_contrast_7': [],\n",
    "    'std_spec_contrast_7': [],\n",
    "    'var_spec_contrast_7': [],\n",
    "    \n",
    "    'mean_mfcc_1': [],\n",
    "    'median_mfcc_1': [],\n",
    "    'std_mfcc_1': [],\n",
    "    'var_mfcc_1': [],\n",
    "    \n",
    "    'mean_mfcc_2': [],\n",
    "    'median_mfcc_2': [],\n",
    "    'std_mfcc_2': [],\n",
    "    'var_mfcc_2': [],\n",
    "    \n",
    "    'mean_mfcc_3': [],\n",
    "    'median_mfcc_3': [],\n",
    "    'std_mfcc_3': [],\n",
    "    'var_mfcc_3': [],\n",
    "    \n",
    "    'mean_mfcc_4': [],\n",
    "    'median_mfcc_4': [],\n",
    "    'std_mfcc_4': [],\n",
    "    'var_mfcc_4': [],\n",
    "    \n",
    "    'mean_mfcc_5': [],\n",
    "    'median_mfcc_5': [],\n",
    "    'std_mfcc_5': [],\n",
    "    'var_mfcc_5': [],\n",
    "    \n",
    "    'mean_mfcc_6': [],\n",
    "    'median_mfcc_6': [],\n",
    "    'std_mfcc_6': [],\n",
    "    'var_mfcc_6': [],\n",
    "    \n",
    "    'mean_mfcc_7': [],\n",
    "    'median_mfcc_7': [],\n",
    "    'std_mfcc_7': [],\n",
    "    'var_mfcc_7': [],\n",
    "    \n",
    "    'mean_mfcc_8': [],\n",
    "    'median_mfcc_8': [],\n",
    "    'std_mfcc_8': [],\n",
    "    'var_mfcc_8': [],\n",
    "    \n",
    "    'mean_mfcc_9': [],\n",
    "    'median_mfcc_9': [],\n",
    "    'std_mfcc_9': [],\n",
    "    'var_mfcc_9': [],\n",
    "    \n",
    "    'mean_mfcc_10': [],\n",
    "    'median_mfcc_10': [],\n",
    "    'std_mfcc_10': [],\n",
    "    'var_mfcc_10': [],\n",
    "    \n",
    "    'mean_mfcc_11': [],\n",
    "    'median_mfcc_11': [],\n",
    "    'std_mfcc_11': [],\n",
    "    'var_mfcc_11': [],\n",
    "    \n",
    "    'mean_mfcc_12': [],\n",
    "    'median_mfcc_12': [],\n",
    "    'std_mfcc_12': [],\n",
    "    'var_mfcc_12': [],\n",
    "    \n",
    "    'mean_mfcc_13': [],\n",
    "    'median_mfcc_13': [],\n",
    "    'std_mfcc_13': [],\n",
    "    'var_mfcc_13': [],\n",
    "    \n",
    "    'lpc_1': [],\n",
    "    'lpc_2': [],\n",
    "    'lpc_3': [],\n",
    "    'lpc_4': [],\n",
    "    \n",
    "    'tempo': [],\n",
    "    \n",
    "    'mean_beats': [],\n",
    "    'median_beats': [],\n",
    "    'std_beats': [],\n",
    "    'var_beats': [],\n",
    "    \n",
    "    'mean_beats_timestamp': [],\n",
    "    'median_beats_timestamp': [],\n",
    "    'std_beats_timestamp': [],\n",
    "    'var_beats_timestamp': [],\n",
    "    \n",
    "    'mean_db4_cA4': [],\n",
    "    'median_db4_cA4': [],\n",
    "    'std_db4_cA4': [],\n",
    "    'var_db4_cA4': [],\n",
    "    'sb_energy_db4_cA4': [],\n",
    "    'skewness_db4_cA4': [],\n",
    "    \n",
    "    'mean_db4_cD4': [],\n",
    "    'median_db4_cD4': [],\n",
    "    'std_db4_cD4': [],\n",
    "    'var_db4_cD4': [],\n",
    "    'sb_energy_db4_cD4': [],\n",
    "    'skewness_db4_cD4': [],\n",
    "    \n",
    "    'mean_db4_cD3': [],\n",
    "    'median_db4_cD3': [],\n",
    "    'std_db4_cD3': [],\n",
    "    'var_db4_cD3': [],\n",
    "    'sb_energy_db4_cD3': [],\n",
    "    'skewness_db4_cD3': [],\n",
    "    \n",
    "    'mean_db4_cD2': [],\n",
    "    'median_db4_cD2': [],\n",
    "    'std_db4_cD2': [],\n",
    "    'var_db4_cD2': [],\n",
    "    'sb_energy_db4_cD2': [],\n",
    "    'skewness_db4_cD2': [],\n",
    "    \n",
    "    'mean_db4_cD1': [],\n",
    "    'median_db4_cD1': [],\n",
    "    'std_db4_cD1': [],\n",
    "    'var_db4_cD1': [],\n",
    "    'sb_energy_db4_cD1': [],\n",
    "    'skewness_db4_cD1': [],\n",
    "    \n",
    "    'mean_db5_cA4': [],\n",
    "    'median_db5_cA4': [],\n",
    "    'std_db5_cA4': [],\n",
    "    'var_db5_cA4': [],\n",
    "    'sb_energy_db5_cA4': [],\n",
    "    'skewness_db5_cA4': [],\n",
    "    \n",
    "    'mean_db5_cD4': [],\n",
    "    'median_db5_cD4': [],\n",
    "    'std_db5_cD4': [],\n",
    "    'var_db5_cD4': [],\n",
    "    'sb_energy_db5_cD4': [],\n",
    "    'skewness_db5_cD4': [],\n",
    "    \n",
    "    'mean_db5_cD3': [],\n",
    "    'median_db5_cD3': [],\n",
    "    'std_db5_cD3': [],\n",
    "    'var_db5_cD3': [],\n",
    "    'sb_energy_db5_cD3': [],\n",
    "    'skewness_db5_cD3': [],\n",
    "    \n",
    "    'mean_db5_cD2': [],\n",
    "    'median_db5_cD2': [],\n",
    "    'std_db5_cD2': [],\n",
    "    'var_db5_cD2': [],\n",
    "    'sb_energy_db5_cD2': [],\n",
    "    'skewness_db5_cD2': [],\n",
    "    \n",
    "    'mean_db5_cD1': [],\n",
    "    'median_db5_cD1': [],\n",
    "    'std_db5_cD1': [],\n",
    "    'var_db5_cD1': [],\n",
    "    'sb_energy_db5_cD1': [],\n",
    "    'skewness_db5_cD1': [],\n",
    "    \n",
    "    'mean_db8_cA7': [],\n",
    "    'median_db8_cA7': [],\n",
    "    'std_db8_cA7': [],\n",
    "    'var_db8_cA7': [],\n",
    "    'sb_energy_db8_cA7': [],\n",
    "    'skewness_db8_cA7': [],\n",
    "    \n",
    "    'mean_db8_cD7': [],\n",
    "    'median_db8_cD7': [],\n",
    "    'std_db8_cD7': [],\n",
    "    'var_db8_cD7': [],\n",
    "    'sb_energy_db8_cD7': [],\n",
    "    'skewness_db8_cD7': [],\n",
    "    \n",
    "    'mean_db8_cD6': [],\n",
    "    'median_db8_cD6': [],\n",
    "    'std_db8_cD6': [],\n",
    "    'var_db8_cD6': [],\n",
    "    'sb_energy_db8_cD6': [],\n",
    "    'skewness_db8_cD6': [],\n",
    "    \n",
    "    'mean_db8_cD5': [],\n",
    "    'median_db8_cD5': [],\n",
    "    'std_db8_cD5': [],\n",
    "    'var_db8_cD5': [],\n",
    "    'sb_energy_db8_cD5': [],\n",
    "    'skewness_db8_cD5': [],\n",
    "    \n",
    "    'mean_db8_cD4': [],\n",
    "    'median_db8_cD4': [],\n",
    "    'std_db8_cD4': [],\n",
    "    'var_db8_cD4': [],\n",
    "    'sb_energy_db8_cD4': [],\n",
    "    'skewness_db8_cD4': [],\n",
    "    \n",
    "    'mean_db8_cD3': [],\n",
    "    'median_db8_cD3': [],\n",
    "    'std_db8_cD3': [],\n",
    "    'var_db8_cD3': [],\n",
    "    'sb_energy_db8_cD3': [],\n",
    "    'skewness_db8_cD3': [],\n",
    "    \n",
    "    'mean_db8_cD2': [],\n",
    "    'median_db8_cD2': [],\n",
    "    'std_db8_cD2': [],\n",
    "    'var_db8_cD2': [],\n",
    "    'sb_energy_db8_cD2': [],\n",
    "    'skewness_db8_cD2': [],\n",
    "    \n",
    "    'mean_db8_cD1': [],\n",
    "    'median_db8_cD1': [],\n",
    "    'std_db8_cD1': [],\n",
    "    'var_db8_cD1': [],\n",
    "    'sb_energy_db8_cD1': [],\n",
    "    'skewness_db8_cD1': [],\n",
    "    \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_features(dataframe, file, genre_label, data_source):\n",
    "    '''\n",
    "    This function takes a datafame, an audio file (check librosa for acceptable formats),\n",
    "    and genre label. It extract features from the audio and returns the datafram with \n",
    "    the new row appended.\n",
    "    \n",
    "    Timbral, rhythmic, and wavelet features are extracted excluding log-mel and mel-spectogram.\n",
    "    \n",
    "    Parameters:\n",
    "    file (File or str): an audio file or file path.\n",
    "    genre_label (str): audio genre label\n",
    "    '''\n",
    "    # get sample rate of audio file\n",
    "    sample_rate = librosa.core.get_samplerate(file.path)\n",
    "\n",
    "    # load audio file as time series\n",
    "    time_series, _ = librosa.core.load(file.path, sample_rate)\n",
    "\n",
    "\n",
    "    # compute timbral features\n",
    "    # compute spectral centroid\n",
    "    spec_centroid = librosa.feature.spectral_centroid(time_series, sample_rate)\n",
    "    stats_spec_centroid = stats(spec_centroid)\n",
    "\n",
    "    # compute spectral roll-off\n",
    "    spec_rolloff = librosa.feature.spectral_rolloff(time_series, sample_rate)\n",
    "    stats_spec_rolloff = stats(spec_rolloff)\n",
    "\n",
    "    # compute zero crossing rate\n",
    "    zcr = librosa.feature.zero_crossing_rate(time_series)\n",
    "    stats_zcr = stats(zcr)\n",
    "\n",
    "    # compute spectral bandwidth\n",
    "    spec_bw = librosa.feature.spectral_bandwidth(time_series, sample_rate)\n",
    "    stats_spec_bw = stats(spec_bw[0])\n",
    "\n",
    "    # compute spectral contrast\n",
    "    spec_contrast = librosa.feature.spectral_contrast(time_series, sample_rate)\n",
    "    stats_spec_contrast_1 = stats(spec_contrast[0])\n",
    "    stats_spec_contrast_2 = stats(spec_contrast[1])\n",
    "    stats_spec_contrast_3 = stats(spec_contrast[2])\n",
    "    stats_spec_contrast_4 = stats(spec_contrast[3])\n",
    "    stats_spec_contrast_5 = stats(spec_contrast[4])\n",
    "    stats_spec_contrast_6 = stats(spec_contrast[5])\n",
    "    stats_spec_contrast_7 = stats(spec_contrast[6])\n",
    "\n",
    "    # compute 13 mel-frequency cepstral coefficients\n",
    "    mfcc = librosa.feature.mfcc(time_series, sample_rate, n_mfcc=13)\n",
    "    stat_mfcc_1 = stats(mfcc[0])\n",
    "    stat_mfcc_2 = stats(mfcc[1])\n",
    "    stat_mfcc_3 = stats(mfcc[2])\n",
    "    stat_mfcc_4 = stats(mfcc[3])\n",
    "    stat_mfcc_5 = stats(mfcc[4])\n",
    "    stat_mfcc_6 = stats(mfcc[5])\n",
    "    stat_mfcc_7 = stats(mfcc[6])\n",
    "    stat_mfcc_8 = stats(mfcc[7])\n",
    "    stat_mfcc_9 = stats(mfcc[8])\n",
    "    stat_mfcc_10 = stats(mfcc[9])\n",
    "    stat_mfcc_11 = stats(mfcc[10])\n",
    "    stat_mfcc_12 = stats(mfcc[11])\n",
    "    stat_mfcc_13 = stats(mfcc[12])\n",
    "\n",
    "    # compute 3rd order linear prediction coefficients\n",
    "    lpc = librosa.lpc(time_series, 3)\n",
    "\n",
    "\n",
    "    # compute rhythmic features\n",
    "    # compute tempo & beats\n",
    "    tempo, beats = librosa.beat.beat_track(time_series, sample_rate)\n",
    "    stats_beats = stats(beats)\n",
    "\n",
    "    # compute timestamps from beats\n",
    "    beats_timestamp = librosa.frames_to_time(beats, sample_rate)\n",
    "    stats_beats_timestamp = stats(beats_timestamp)\n",
    "\n",
    "\n",
    "    # compute wavelet features\n",
    "    # compute coefficients for Db4 at level 4 decomposition\n",
    "    db4_coeffs = pywt.wavedec(time_series, 'db4', level=4)\n",
    "    db4_cA4, db4_cD4, db4_cD3, db4_cD2, db4_cD1 = db4_coeffs\n",
    "    stats_db4_cA4 = {**stats(db4_cA4), **extra_stats(db4_cA4)}\n",
    "    stats_db4_cD4 = {**stats(db4_cD4), **extra_stats(db4_cD4)}\n",
    "    stats_db4_cD3 = {**stats(db4_cD3), **extra_stats(db4_cD3)}\n",
    "    stats_db4_cD2 = {**stats(db4_cD2), **extra_stats(db4_cD2)}\n",
    "    stats_db4_cD1 = {**stats(db4_cD1), **extra_stats(db4_cD1)}\n",
    "\n",
    "    # compute coefficients for Db5 at level 4 decomposition\n",
    "    db5_coeffs = pywt.wavedec(time_series, 'db5', level=4)\n",
    "    db5_cA4, db5_cD4, db5_cD3, db5_cD2, db5_cD1 = db5_coeffs\n",
    "    stats_db5_cA4 = {**stats(db5_cA4), **extra_stats(db5_cA4)}\n",
    "    stats_db5_cD4 = {**stats(db5_cD4), **extra_stats(db5_cD4)}\n",
    "    stats_db5_cD3 = {**stats(db5_cD3), **extra_stats(db5_cD3)}\n",
    "    stats_db5_cD2 = {**stats(db5_cD2), **extra_stats(db5_cD2)}\n",
    "    stats_db5_cD1 = {**stats(db5_cD1), **extra_stats(db5_cD1)}\n",
    "\n",
    "    # compute coefficients for Db8 at level 7 decomposition\n",
    "    db8_coeffs = pywt.wavedec(time_series, 'db4', level=7)\n",
    "    db8_cA7, db8_cD7, db8_cD6, db8_cD5, db8_cD4, db8_cD3, db8_cD2, db8_cD1 = db8_coeffs\n",
    "    stats_db8_cA7 = {**stats(db8_cA7), **extra_stats(db8_cA7)}\n",
    "    stats_db8_cD7 = {**stats(db8_cD7), **extra_stats(db8_cD7)}\n",
    "    stats_db8_cD6 = {**stats(db8_cD6), **extra_stats(db8_cD6)}\n",
    "    stats_db8_cD5 = {**stats(db8_cD5), **extra_stats(db8_cD5)}\n",
    "    stats_db8_cD4 = {**stats(db8_cD4), **extra_stats(db8_cD4)}\n",
    "    stats_db8_cD3 = {**stats(db8_cD3), **extra_stats(db8_cD3)}\n",
    "    stats_db8_cD2 = {**stats(db8_cD2), **extra_stats(db8_cD2)}\n",
    "    stats_db8_cD1 = {**stats(db8_cD1), **extra_stats(db8_cD1)}\n",
    "    \n",
    "    # create new row\n",
    "    new_row = {\n",
    "    'genre_label': genre_label,\n",
    "    'data_source': data_source,\n",
    "        \n",
    "    'mean_spec_centroid': stats_spec_centroid['mean'],\n",
    "    'median_spec_centroid': stats_spec_centroid['median'],\n",
    "    'std_spec_centroid': stats_spec_centroid['std'],\n",
    "    'var_spec_centroid': stats_spec_centroid['var'],\n",
    "    \n",
    "    'mean_spec_rolloff': stats_spec_rolloff['mean'],\n",
    "    'median_spec_rolloff': stats_spec_rolloff['median'],\n",
    "    'std_spec_rolloff': stats_spec_rolloff['std'],\n",
    "    'var_spec_rolloff': stats_spec_rolloff['var'],\n",
    "    \n",
    "    'mean_zcr': stats_zcr['mean'],\n",
    "    'median_zcr': stats_zcr['median'],\n",
    "    'std_zcr': stats_zcr['std'],\n",
    "    'var_zcr': stats_zcr['var'],\n",
    "    \n",
    "    'mean_spec_bw': stats_spec_bw['mean'],\n",
    "    'median_spec_bw': stats_spec_bw['median'],\n",
    "    'std_spec_bw': stats_spec_bw['std'],\n",
    "    'var_spec_bw': stats_spec_bw['var'],\n",
    "    \n",
    "    'mean_spec_contrast_1': stats_spec_contrast_1['mean'],\n",
    "    'median_spec_contrast_1': stats_spec_contrast_1['median'],\n",
    "    'std_spec_contrast_1': stats_spec_contrast_1['std'],\n",
    "    'var_spec_contrast_1': stats_spec_contrast_1['var'],\n",
    "    \n",
    "    'mean_spec_contrast_2': stats_spec_contrast_2['mean'],\n",
    "    'median_spec_contrast_2': stats_spec_contrast_2['median'],\n",
    "    'std_spec_contrast_2': stats_spec_contrast_2['std'],\n",
    "    'var_spec_contrast_2': stats_spec_contrast_2['var'],\n",
    "    \n",
    "    'mean_spec_contrast_3': stats_spec_contrast_3['mean'],\n",
    "    'median_spec_contrast_3': stats_spec_contrast_3['median'],\n",
    "    'std_spec_contrast_3': stats_spec_contrast_3['std'],\n",
    "    'var_spec_contrast_3': stats_spec_contrast_3['var'],\n",
    "    \n",
    "    'mean_spec_contrast_4': stats_spec_contrast_4['mean'],\n",
    "    'median_spec_contrast_4': stats_spec_contrast_4['median'],\n",
    "    'std_spec_contrast_4': stats_spec_contrast_4['std'],\n",
    "    'var_spec_contrast_4': stats_spec_contrast_4['var'],\n",
    "    \n",
    "    'mean_spec_contrast_5': stats_spec_contrast_5['mean'],\n",
    "    'median_spec_contrast_5': stats_spec_contrast_5['median'],\n",
    "    'std_spec_contrast_5': stats_spec_contrast_5['std'],\n",
    "    'var_spec_contrast_5': stats_spec_contrast_5['var'],\n",
    "    \n",
    "    'mean_spec_contrast_6': stats_spec_contrast_6['mean'],\n",
    "    'median_spec_contrast_6': stats_spec_contrast_6['median'],\n",
    "    'std_spec_contrast_6': stats_spec_contrast_6['std'],\n",
    "    'var_spec_contrast_6': stats_spec_contrast_6['var'],\n",
    "    \n",
    "    'mean_spec_contrast_7': stats_spec_contrast_7['mean'],\n",
    "    'median_spec_contrast_7': stats_spec_contrast_7['median'],\n",
    "    'std_spec_contrast_7': stats_spec_contrast_7['std'],\n",
    "    'var_spec_contrast_7': stats_spec_contrast_7['var'],\n",
    "    \n",
    "    'mean_mfcc_1': stat_mfcc_1['mean'],\n",
    "    'median_mfcc_1': stat_mfcc_1['median'],\n",
    "    'std_mfcc_1': stat_mfcc_1['std'],\n",
    "    'var_mfcc_1': stat_mfcc_1['var'],\n",
    "    \n",
    "    'mean_mfcc_2': stat_mfcc_2['mean'],\n",
    "    'median_mfcc_2': stat_mfcc_2['median'],\n",
    "    'std_mfcc_2': stat_mfcc_2['std'],\n",
    "    'var_mfcc_2': stat_mfcc_2['var'],\n",
    "    \n",
    "    'mean_mfcc_3': stat_mfcc_3['mean'],\n",
    "    'median_mfcc_3': stat_mfcc_3['median'],\n",
    "    'std_mfcc_3': stat_mfcc_3['std'],\n",
    "    'var_mfcc_3': stat_mfcc_3['var'],\n",
    "    \n",
    "    'mean_mfcc_4': stat_mfcc_4['mean'],\n",
    "    'median_mfcc_4': stat_mfcc_4['median'],\n",
    "    'std_mfcc_4': stat_mfcc_4['std'],\n",
    "    'var_mfcc_4': stat_mfcc_4['var'],\n",
    "    \n",
    "    'mean_mfcc_5': stat_mfcc_5['mean'],\n",
    "    'median_mfcc_5': stat_mfcc_5['median'],\n",
    "    'std_mfcc_5': stat_mfcc_5['std'],\n",
    "    'var_mfcc_5': stat_mfcc_5['var'],\n",
    "    \n",
    "    'mean_mfcc_6': stat_mfcc_6['mean'],\n",
    "    'median_mfcc_6': stat_mfcc_6['median'],\n",
    "    'std_mfcc_6': stat_mfcc_6['std'],\n",
    "    'var_mfcc_6': stat_mfcc_6['var'],\n",
    "    \n",
    "    'mean_mfcc_7': stat_mfcc_7['mean'],\n",
    "    'median_mfcc_7': stat_mfcc_7['median'],\n",
    "    'std_mfcc_7': stat_mfcc_7['std'],\n",
    "    'var_mfcc_7': stat_mfcc_7['var'],\n",
    "    \n",
    "    'mean_mfcc_8': stat_mfcc_8['mean'],\n",
    "    'median_mfcc_8': stat_mfcc_8['median'],\n",
    "    'std_mfcc_8': stat_mfcc_8['std'],\n",
    "    'var_mfcc_8':stat_mfcc_8['var'],\n",
    "    \n",
    "    'mean_mfcc_9': stat_mfcc_9['mean'],\n",
    "    'median_mfcc_9': stat_mfcc_9['median'],\n",
    "    'std_mfcc_9': stat_mfcc_9['std'],\n",
    "    'var_mfcc_9': stat_mfcc_9['var'],\n",
    "    \n",
    "    'mean_mfcc_10': stat_mfcc_10['mean'],\n",
    "    'median_mfcc_10': stat_mfcc_10['median'],\n",
    "    'std_mfcc_10': stat_mfcc_10['std'],\n",
    "    'var_mfcc_10': stat_mfcc_10['var'],\n",
    "    \n",
    "    'mean_mfcc_11': stat_mfcc_11['mean'],\n",
    "    'median_mfcc_11': stat_mfcc_11['median'],\n",
    "    'std_mfcc_11': stat_mfcc_11['std'],\n",
    "    'var_mfcc_11': stat_mfcc_11['var'],\n",
    "    \n",
    "    'mean_mfcc_12': stat_mfcc_12['mean'],\n",
    "    'median_mfcc_12': stat_mfcc_12['median'],\n",
    "    'std_mfcc_12': stat_mfcc_12['std'],\n",
    "    'var_mfcc_12': stat_mfcc_12['var'],\n",
    "    \n",
    "    'mean_mfcc_13': stat_mfcc_13['mean'],\n",
    "    'median_mfcc_13': stat_mfcc_13['median'],\n",
    "    'std_mfcc_13': stat_mfcc_13['std'],\n",
    "    'var_mfcc_13': stat_mfcc_13['var'],\n",
    "    \n",
    "    'lpc_1': lpc[0],\n",
    "    'lpc_2': lpc[1],\n",
    "    'lpc_3': lpc[2],\n",
    "    'lpc_4': lpc[3],\n",
    "    \n",
    "    'tempo': tempo,\n",
    "    \n",
    "    'mean_beats': stats_beats['mean'],\n",
    "    'median_beats': stats_beats['median'],\n",
    "    'std_beats': stats_beats['std'],\n",
    "    'var_beats': stats_beats['var'],\n",
    "    \n",
    "    'mean_beats_timestamp': stats_beats_timestamp['mean'],\n",
    "    'median_beats_timestamp': stats_beats_timestamp['median'],\n",
    "    'std_beats_timestamp': stats_beats_timestamp['std'],\n",
    "    'var_beats_timestamp': stats_beats_timestamp['var'],\n",
    "    \n",
    "    'mean_db4_cA4': stats_db4_cA4['mean'],\n",
    "    'median_db4_cA4': stats_db4_cA4['median'],\n",
    "    'std_db4_cA4': stats_db4_cA4['std'],\n",
    "    'var_db4_cA4': stats_db4_cA4['var'],\n",
    "    'sb_energy_db4_cA4': stats_db4_cA4['sb_energy'],\n",
    "    'skewness_db4_cA4': stats_db4_cA4['skewness'],\n",
    "    \n",
    "    'mean_db4_cD4': stats_db4_cD4['mean'],\n",
    "    'median_db4_cD4': stats_db4_cD4['median'],\n",
    "    'std_db4_cD4': stats_db4_cD4['std'],\n",
    "    'var_db4_cD4': stats_db4_cD4['var'],\n",
    "    'sb_energy_db4_cD4': stats_db4_cD4['sb_energy'],\n",
    "    'skewness_db4_cD4': stats_db4_cD4['skewness'],\n",
    "    \n",
    "    'mean_db4_cD3': stats_db4_cD3['mean'],\n",
    "    'median_db4_cD3': stats_db4_cD3['median'],\n",
    "    'std_db4_cD3': stats_db4_cD3['std'],\n",
    "    'var_db4_cD3': stats_db4_cD3['var'],\n",
    "    'sb_energy_db4_cD3': stats_db4_cD3['sb_energy'],\n",
    "    'skewness_db4_cD3': stats_db4_cD3['skewness'],\n",
    "    \n",
    "    'mean_db4_cD2': stats_db4_cD2['mean'],\n",
    "    'median_db4_cD2': stats_db4_cD2['median'],\n",
    "    'std_db4_cD2': stats_db4_cD2['std'],\n",
    "    'var_db4_cD2': stats_db4_cD2['var'],\n",
    "    'sb_energy_db4_cD2': stats_db4_cD2['sb_energy'],\n",
    "    'skewness_db4_cD2': stats_db4_cD2['skewness'],\n",
    "    \n",
    "    'mean_db4_cD1': stats_db4_cD1['mean'],\n",
    "    'median_db4_cD1': stats_db4_cD1['median'],\n",
    "    'std_db4_cD1': stats_db4_cD1['std'],\n",
    "    'var_db4_cD1': stats_db4_cD1['var'],\n",
    "    'sb_energy_db4_cD1': stats_db4_cD1['sb_energy'],\n",
    "    'skewness_db4_cD1': stats_db4_cD1['skewness'],\n",
    "    \n",
    "    'mean_db5_cA4': stats_db5_cA4['mean'],\n",
    "    'median_db5_cA4': stats_db5_cA4['median'],\n",
    "    'std_db5_cA4': stats_db5_cA4['std'],\n",
    "    'var_db5_cA4': stats_db5_cA4['var'],\n",
    "    'sb_energy_db5_cA4': stats_db5_cA4['sb_energy'],\n",
    "    'skewness_db5_cA4': stats_db5_cA4['skewness'],\n",
    "    \n",
    "    'mean_db5_cD4': stats_db5_cD4['mean'],\n",
    "    'median_db5_cD4': stats_db5_cD4['median'],\n",
    "    'std_db5_cD4': stats_db5_cD4['std'],\n",
    "    'var_db5_cD4': stats_db5_cD4['var'],\n",
    "    'sb_energy_db5_cD4': stats_db5_cD4['sb_energy'],\n",
    "    'skewness_db5_cD4': stats_db5_cD4['skewness'],\n",
    "    \n",
    "    'mean_db5_cD3': stats_db5_cD3['mean'],\n",
    "    'median_db5_cD3': stats_db5_cD3['median'],\n",
    "    'std_db5_cD3': stats_db5_cD3['std'],\n",
    "    'var_db5_cD3': stats_db5_cD3['var'],\n",
    "    'sb_energy_db5_cD3': stats_db5_cD3['sb_energy'],\n",
    "    'skewness_db5_cD3': stats_db5_cD3['skewness'],\n",
    "    \n",
    "    'mean_db5_cD2': stats_db5_cD2['mean'],\n",
    "    'median_db5_cD2': stats_db5_cD2['median'],\n",
    "    'std_db5_cD2': stats_db5_cD2['std'],\n",
    "    'var_db5_cD2': stats_db5_cD2['var'],\n",
    "    'sb_energy_db5_cD2': stats_db5_cD2['sb_energy'],\n",
    "    'skewness_db5_cD2': stats_db5_cD2['skewness'],\n",
    "    \n",
    "    'mean_db5_cD1': stats_db5_cD1['mean'],\n",
    "    'median_db5_cD1': stats_db5_cD1['median'],\n",
    "    'std_db5_cD1': stats_db5_cD1['std'],\n",
    "    'var_db5_cD1': stats_db5_cD1['var'],\n",
    "    'sb_energy_db5_cD1': stats_db5_cD1['sb_energy'],\n",
    "    'skewness_db5_cD1': stats_db5_cD1['skewness'],\n",
    "    \n",
    "    'mean_db8_cA7': stats_db8_cA7['mean'],\n",
    "    'median_db8_cA7': stats_db8_cA7['median'],\n",
    "    'std_db8_cA7': stats_db8_cA7['std'],\n",
    "    'var_db8_cA7': stats_db8_cA7['var'],\n",
    "    'sb_energy_db8_cA7': stats_db8_cA7['sb_energy'],\n",
    "    'skewness_db8_cA7': stats_db8_cA7['skewness'],\n",
    "    \n",
    "    'mean_db8_cD7': stats_db8_cD7['mean'],\n",
    "    'median_db8_cD7': stats_db8_cD7['median'],\n",
    "    'std_db8_cD7': stats_db8_cD7['std'],\n",
    "    'var_db8_cD7': stats_db8_cD7['var'],\n",
    "    'sb_energy_db8_cD7': stats_db8_cD7['sb_energy'],\n",
    "    'skewness_db8_cD7': stats_db8_cD7['skewness'],\n",
    "    \n",
    "    'mean_db8_cD6': stats_db8_cD6['mean'],\n",
    "    'median_db8_cD6': stats_db8_cD6['median'],\n",
    "    'std_db8_cD6': stats_db8_cD6['std'],\n",
    "    'var_db8_cD6': stats_db8_cD6['var'],\n",
    "    'sb_energy_db8_cD6': stats_db8_cD6['sb_energy'],\n",
    "    'skewness_db8_cD6': stats_db8_cD6['skewness'],\n",
    "    \n",
    "    'mean_db8_cD5': stats_db8_cD5['mean'],\n",
    "    'median_db8_cD5': stats_db8_cD5['median'],\n",
    "    'std_db8_cD5': stats_db8_cD5['std'],\n",
    "    'var_db8_cD5': stats_db8_cD5['var'],\n",
    "    'sb_energy_db8_cD5': stats_db8_cD5['sb_energy'],\n",
    "    'skewness_db8_cD5': stats_db8_cD5['skewness'],\n",
    "    \n",
    "    'mean_db8_cD4': stats_db8_cD4['mean'],\n",
    "    'median_db8_cD4': stats_db8_cD4['median'],\n",
    "    'std_db8_cD4': stats_db8_cD4['std'],\n",
    "    'var_db8_cD4': stats_db8_cD4['var'],\n",
    "    'sb_energy_db8_cD4': stats_db8_cD4['sb_energy'],\n",
    "    'skewness_db8_cD4': stats_db8_cD4['skewness'],\n",
    "    \n",
    "    'mean_db8_cD3': stats_db8_cD3['mean'],\n",
    "    'median_db8_cD3': stats_db8_cD3['median'],\n",
    "    'std_db8_cD3': stats_db8_cD3['std'],\n",
    "    'var_db8_cD3': stats_db8_cD3['var'],\n",
    "    'sb_energy_db8_cD3': stats_db8_cD3['sb_energy'],\n",
    "    'skewness_db8_cD3': stats_db8_cD3['skewness'],\n",
    "    \n",
    "    'mean_db8_cD2': stats_db8_cD2['mean'],\n",
    "    'median_db8_cD2': stats_db8_cD2['median'],\n",
    "    'std_db8_cD2': stats_db8_cD2['std'],\n",
    "    'var_db8_cD2': stats_db8_cD2['var'],\n",
    "    'sb_energy_db8_cD2': stats_db8_cD2['sb_energy'],\n",
    "    'skewness_db8_cD2': stats_db8_cD2['skewness'],\n",
    "    \n",
    "    'mean_db8_cD1': stats_db8_cD1['mean'],\n",
    "    'median_db8_cD1': stats_db8_cD1['median'],\n",
    "    'std_db8_cD1': stats_db8_cD1['std'],\n",
    "    'var_db8_cD1': stats_db8_cD1['var'],\n",
    "    'sb_energy_db8_cD1': stats_db8_cD1['sb_energy'],\n",
    "    'skewness_db8_cD1': stats_db8_cD1['skewness'],\n",
    "    }\n",
    "    \n",
    "    # append new row\n",
    "    dataframe = dataframe.append(new_row, ignore_index=True)\n",
    "    \n",
    "    print('appended features extracted from ' + str(file))\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GTZAN\n",
    "# extract features\n",
    "\n",
    "print('extracting features from gtzan dataset')\n",
    "\n",
    "for file in os.scandir(MOUNTED_DATASET_PATH + '/gtzan/wavfiles'):\n",
    "    if file.is_file():\n",
    "        genre_label = str(file.name).split('.')[0]\n",
    "        dataframe = extract_audio_features(dataframe, file, genre_label, 'gtzan')\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FMA\n",
    "# collect track id and genres of tracks in the small subset.\n",
    "\n",
    "print('collecting track id and genres of tracks in the small subset of fma dataset')\n",
    "\n",
    "tracks = load(MOUNTED_DATASET_PATH + '/fma_metadata/tracks.csv')\n",
    "fma_full = tracks[[('set', 'subset'), ('track', 'genre_top')]]\n",
    "small_subset = fma_full[('set', 'subset')] == 'small'\n",
    "fma_small = fma_full[small_subset]\n",
    "fma_small = pd.DataFrame({\n",
    "    'subset': fma_small[('set', 'subset')],\n",
    "    'label': fma_small[('track', 'genre_top')]\n",
    "})\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FMA\n",
    "# extract features\n",
    "\n",
    "print('extracting features from fma dataset')\n",
    "\n",
    "for directory in os.scandir(MOUNTED_DATASET_PATH + '/fma_small'):\n",
    "    if directory.is_dir():\n",
    "        for file in os.scandir(directory):\n",
    "            if file.is_file():\n",
    "                track_id = int(file.name[:-4].lstrip('0'))\n",
    "                genre_label = fma_small.at[track_id, 'label'].lower().replace('-', '')\n",
    "                dataframe = extract_audio_features(dataframe, file, genre_label, 'fma')\n",
    "\n",
    "print(done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genre_label</th>\n",
       "      <th>data_source</th>\n",
       "      <th>mean_spec_centroid</th>\n",
       "      <th>median_spec_centroid</th>\n",
       "      <th>std_spec_centroid</th>\n",
       "      <th>var_spec_centroid</th>\n",
       "      <th>mean_spec_rolloff</th>\n",
       "      <th>median_spec_rolloff</th>\n",
       "      <th>std_spec_rolloff</th>\n",
       "      <th>var_spec_rolloff</th>\n",
       "      <th>...</th>\n",
       "      <th>std_db8_cD2</th>\n",
       "      <th>var_db8_cD2</th>\n",
       "      <th>sb_energy_db8_cD2</th>\n",
       "      <th>skewness_db8_cD2</th>\n",
       "      <th>mean_db8_cD1</th>\n",
       "      <th>median_db8_cD1</th>\n",
       "      <th>std_db8_cD1</th>\n",
       "      <th>var_db8_cD1</th>\n",
       "      <th>sb_energy_db8_cD1</th>\n",
       "      <th>skewness_db8_cD1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>blues</td>\n",
       "      <td>gtzan</td>\n",
       "      <td>1784.165850</td>\n",
       "      <td>1739.018858</td>\n",
       "      <td>360.241675</td>\n",
       "      <td>1.297741e+05</td>\n",
       "      <td>3805.839606</td>\n",
       "      <td>3649.877930</td>\n",
       "      <td>949.476395</td>\n",
       "      <td>9.015054e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042811</td>\n",
       "      <td>0.001833</td>\n",
       "      <td>0.029654</td>\n",
       "      <td>0.010215</td>\n",
       "      <td>-2.474470e-08</td>\n",
       "      <td>1.208096e-06</td>\n",
       "      <td>0.014643</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.009321</td>\n",
       "      <td>0.022624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>blues</td>\n",
       "      <td>gtzan</td>\n",
       "      <td>1530.176679</td>\n",
       "      <td>1462.001794</td>\n",
       "      <td>613.066125</td>\n",
       "      <td>3.758501e+05</td>\n",
       "      <td>3550.522098</td>\n",
       "      <td>3219.213867</td>\n",
       "      <td>1725.657379</td>\n",
       "      <td>2.977893e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025257</td>\n",
       "      <td>0.000638</td>\n",
       "      <td>0.013157</td>\n",
       "      <td>0.088051</td>\n",
       "      <td>-2.979098e-09</td>\n",
       "      <td>3.874273e-06</td>\n",
       "      <td>0.012940</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>0.006530</td>\n",
       "      <td>-0.041386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>blues</td>\n",
       "      <td>gtzan</td>\n",
       "      <td>1552.811865</td>\n",
       "      <td>1510.972589</td>\n",
       "      <td>395.559911</td>\n",
       "      <td>1.564676e+05</td>\n",
       "      <td>3042.260232</td>\n",
       "      <td>2906.982422</td>\n",
       "      <td>885.457204</td>\n",
       "      <td>7.840345e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059410</td>\n",
       "      <td>0.003530</td>\n",
       "      <td>0.037085</td>\n",
       "      <td>-0.111269</td>\n",
       "      <td>1.485183e-08</td>\n",
       "      <td>-1.273306e-05</td>\n",
       "      <td>0.015932</td>\n",
       "      <td>0.000254</td>\n",
       "      <td>0.009553</td>\n",
       "      <td>0.060238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>blues</td>\n",
       "      <td>gtzan</td>\n",
       "      <td>1070.106615</td>\n",
       "      <td>1003.841572</td>\n",
       "      <td>429.366909</td>\n",
       "      <td>1.843559e+05</td>\n",
       "      <td>2184.745799</td>\n",
       "      <td>1905.688477</td>\n",
       "      <td>1221.963322</td>\n",
       "      <td>1.493194e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016831</td>\n",
       "      <td>0.000283</td>\n",
       "      <td>0.008966</td>\n",
       "      <td>-0.151483</td>\n",
       "      <td>4.115053e-09</td>\n",
       "      <td>1.085941e-06</td>\n",
       "      <td>0.006558</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.003244</td>\n",
       "      <td>0.028574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>blues</td>\n",
       "      <td>gtzan</td>\n",
       "      <td>1835.004266</td>\n",
       "      <td>1732.517594</td>\n",
       "      <td>586.003361</td>\n",
       "      <td>3.433999e+05</td>\n",
       "      <td>3579.757627</td>\n",
       "      <td>3542.211914</td>\n",
       "      <td>1254.184130</td>\n",
       "      <td>1.572978e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058021</td>\n",
       "      <td>0.003366</td>\n",
       "      <td>0.030722</td>\n",
       "      <td>-0.046622</td>\n",
       "      <td>-1.857917e-08</td>\n",
       "      <td>1.026267e-05</td>\n",
       "      <td>0.020510</td>\n",
       "      <td>0.000421</td>\n",
       "      <td>0.009280</td>\n",
       "      <td>-0.022495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>blues</td>\n",
       "      <td>gtzan</td>\n",
       "      <td>1831.993940</td>\n",
       "      <td>1471.018339</td>\n",
       "      <td>1015.126775</td>\n",
       "      <td>1.030482e+06</td>\n",
       "      <td>3481.517592</td>\n",
       "      <td>2906.982422</td>\n",
       "      <td>1809.541367</td>\n",
       "      <td>3.274440e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079287</td>\n",
       "      <td>0.006286</td>\n",
       "      <td>0.034646</td>\n",
       "      <td>-0.149979</td>\n",
       "      <td>1.007725e-07</td>\n",
       "      <td>5.273178e-06</td>\n",
       "      <td>0.038599</td>\n",
       "      <td>0.001490</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.074893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>blues</td>\n",
       "      <td>gtzan</td>\n",
       "      <td>1459.366472</td>\n",
       "      <td>1312.423565</td>\n",
       "      <td>661.709451</td>\n",
       "      <td>4.378594e+05</td>\n",
       "      <td>2795.610963</td>\n",
       "      <td>2411.718750</td>\n",
       "      <td>1273.358561</td>\n",
       "      <td>1.621442e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081001</td>\n",
       "      <td>0.006561</td>\n",
       "      <td>0.040158</td>\n",
       "      <td>-0.367995</td>\n",
       "      <td>-8.206952e-08</td>\n",
       "      <td>4.526431e-06</td>\n",
       "      <td>0.020290</td>\n",
       "      <td>0.000412</td>\n",
       "      <td>0.007897</td>\n",
       "      <td>0.259115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>blues</td>\n",
       "      <td>gtzan</td>\n",
       "      <td>1451.667066</td>\n",
       "      <td>1295.280201</td>\n",
       "      <td>670.498443</td>\n",
       "      <td>4.495682e+05</td>\n",
       "      <td>2954.836760</td>\n",
       "      <td>2551.684570</td>\n",
       "      <td>1276.373759</td>\n",
       "      <td>1.629130e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.078624</td>\n",
       "      <td>0.006182</td>\n",
       "      <td>0.030819</td>\n",
       "      <td>-0.039611</td>\n",
       "      <td>-1.246119e-08</td>\n",
       "      <td>7.654016e-06</td>\n",
       "      <td>0.020083</td>\n",
       "      <td>0.000403</td>\n",
       "      <td>0.007329</td>\n",
       "      <td>0.488535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>blues</td>\n",
       "      <td>gtzan</td>\n",
       "      <td>1719.368948</td>\n",
       "      <td>1605.046432</td>\n",
       "      <td>404.082721</td>\n",
       "      <td>1.632828e+05</td>\n",
       "      <td>3782.316288</td>\n",
       "      <td>3499.145508</td>\n",
       "      <td>1123.795795</td>\n",
       "      <td>1.262917e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043566</td>\n",
       "      <td>0.001898</td>\n",
       "      <td>0.029226</td>\n",
       "      <td>0.060827</td>\n",
       "      <td>2.568507e-08</td>\n",
       "      <td>8.145493e-06</td>\n",
       "      <td>0.017052</td>\n",
       "      <td>0.000291</td>\n",
       "      <td>0.010127</td>\n",
       "      <td>-0.033558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>hiphop</td>\n",
       "      <td>fma</td>\n",
       "      <td>3684.235126</td>\n",
       "      <td>3280.994684</td>\n",
       "      <td>1401.500103</td>\n",
       "      <td>1.964203e+06</td>\n",
       "      <td>7418.621807</td>\n",
       "      <td>7256.689453</td>\n",
       "      <td>2287.136883</td>\n",
       "      <td>5.230995e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.065414</td>\n",
       "      <td>0.004279</td>\n",
       "      <td>0.039061</td>\n",
       "      <td>-0.022171</td>\n",
       "      <td>-1.770226e-08</td>\n",
       "      <td>8.129573e-07</td>\n",
       "      <td>0.023415</td>\n",
       "      <td>0.000548</td>\n",
       "      <td>0.012639</td>\n",
       "      <td>-0.008580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>hiphop</td>\n",
       "      <td>fma</td>\n",
       "      <td>2929.221292</td>\n",
       "      <td>2748.272127</td>\n",
       "      <td>1200.472486</td>\n",
       "      <td>1.441134e+06</td>\n",
       "      <td>6356.716473</td>\n",
       "      <td>6481.494141</td>\n",
       "      <td>2210.899958</td>\n",
       "      <td>4.888079e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033730</td>\n",
       "      <td>0.001138</td>\n",
       "      <td>0.022045</td>\n",
       "      <td>-0.013626</td>\n",
       "      <td>5.186427e-08</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.010405</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.006491</td>\n",
       "      <td>0.002405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>pop</td>\n",
       "      <td>fma</td>\n",
       "      <td>2787.250132</td>\n",
       "      <td>2538.038341</td>\n",
       "      <td>798.156083</td>\n",
       "      <td>6.370531e+05</td>\n",
       "      <td>5242.751631</td>\n",
       "      <td>4306.640625</td>\n",
       "      <td>2112.685403</td>\n",
       "      <td>4.463440e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045989</td>\n",
       "      <td>0.002115</td>\n",
       "      <td>0.033668</td>\n",
       "      <td>-0.030781</td>\n",
       "      <td>2.673177e-08</td>\n",
       "      <td>2.884911e-05</td>\n",
       "      <td>0.016135</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>0.009331</td>\n",
       "      <td>-0.012464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>folk</td>\n",
       "      <td>fma</td>\n",
       "      <td>1965.307620</td>\n",
       "      <td>1675.430800</td>\n",
       "      <td>1252.303542</td>\n",
       "      <td>1.568264e+06</td>\n",
       "      <td>4327.390500</td>\n",
       "      <td>3552.978516</td>\n",
       "      <td>3058.106324</td>\n",
       "      <td>9.352014e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012634</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.005332</td>\n",
       "      <td>0.292082</td>\n",
       "      <td>8.272701e-08</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.004892</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.002059</td>\n",
       "      <td>0.003960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13 rows × 219 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   genre_label data_source  mean_spec_centroid  median_spec_centroid  \\\n",
       "0        blues       gtzan         1784.165850           1739.018858   \n",
       "1        blues       gtzan         1530.176679           1462.001794   \n",
       "2        blues       gtzan         1552.811865           1510.972589   \n",
       "3        blues       gtzan         1070.106615           1003.841572   \n",
       "4        blues       gtzan         1835.004266           1732.517594   \n",
       "5        blues       gtzan         1831.993940           1471.018339   \n",
       "6        blues       gtzan         1459.366472           1312.423565   \n",
       "7        blues       gtzan         1451.667066           1295.280201   \n",
       "8        blues       gtzan         1719.368948           1605.046432   \n",
       "9       hiphop         fma         3684.235126           3280.994684   \n",
       "10      hiphop         fma         2929.221292           2748.272127   \n",
       "11         pop         fma         2787.250132           2538.038341   \n",
       "12        folk         fma         1965.307620           1675.430800   \n",
       "\n",
       "    std_spec_centroid  var_spec_centroid  mean_spec_rolloff  \\\n",
       "0          360.241675       1.297741e+05        3805.839606   \n",
       "1          613.066125       3.758501e+05        3550.522098   \n",
       "2          395.559911       1.564676e+05        3042.260232   \n",
       "3          429.366909       1.843559e+05        2184.745799   \n",
       "4          586.003361       3.433999e+05        3579.757627   \n",
       "5         1015.126775       1.030482e+06        3481.517592   \n",
       "6          661.709451       4.378594e+05        2795.610963   \n",
       "7          670.498443       4.495682e+05        2954.836760   \n",
       "8          404.082721       1.632828e+05        3782.316288   \n",
       "9         1401.500103       1.964203e+06        7418.621807   \n",
       "10        1200.472486       1.441134e+06        6356.716473   \n",
       "11         798.156083       6.370531e+05        5242.751631   \n",
       "12        1252.303542       1.568264e+06        4327.390500   \n",
       "\n",
       "    median_spec_rolloff  std_spec_rolloff  var_spec_rolloff  ...  std_db8_cD2  \\\n",
       "0           3649.877930        949.476395      9.015054e+05  ...     0.042811   \n",
       "1           3219.213867       1725.657379      2.977893e+06  ...     0.025257   \n",
       "2           2906.982422        885.457204      7.840345e+05  ...     0.059410   \n",
       "3           1905.688477       1221.963322      1.493194e+06  ...     0.016831   \n",
       "4           3542.211914       1254.184130      1.572978e+06  ...     0.058021   \n",
       "5           2906.982422       1809.541367      3.274440e+06  ...     0.079287   \n",
       "6           2411.718750       1273.358561      1.621442e+06  ...     0.081001   \n",
       "7           2551.684570       1276.373759      1.629130e+06  ...     0.078624   \n",
       "8           3499.145508       1123.795795      1.262917e+06  ...     0.043566   \n",
       "9           7256.689453       2287.136883      5.230995e+06  ...     0.065414   \n",
       "10          6481.494141       2210.899958      4.888079e+06  ...     0.033730   \n",
       "11          4306.640625       2112.685403      4.463440e+06  ...     0.045989   \n",
       "12          3552.978516       3058.106324      9.352014e+06  ...     0.012634   \n",
       "\n",
       "    var_db8_cD2  sb_energy_db8_cD2  skewness_db8_cD2  mean_db8_cD1  \\\n",
       "0      0.001833           0.029654          0.010215 -2.474470e-08   \n",
       "1      0.000638           0.013157          0.088051 -2.979098e-09   \n",
       "2      0.003530           0.037085         -0.111269  1.485183e-08   \n",
       "3      0.000283           0.008966         -0.151483  4.115053e-09   \n",
       "4      0.003366           0.030722         -0.046622 -1.857917e-08   \n",
       "5      0.006286           0.034646         -0.149979  1.007725e-07   \n",
       "6      0.006561           0.040158         -0.367995 -8.206952e-08   \n",
       "7      0.006182           0.030819         -0.039611 -1.246119e-08   \n",
       "8      0.001898           0.029226          0.060827  2.568507e-08   \n",
       "9      0.004279           0.039061         -0.022171 -1.770226e-08   \n",
       "10     0.001138           0.022045         -0.013626  5.186427e-08   \n",
       "11     0.002115           0.033668         -0.030781  2.673177e-08   \n",
       "12     0.000160           0.005332          0.292082  8.272701e-08   \n",
       "\n",
       "    median_db8_cD1  std_db8_cD1  var_db8_cD1  sb_energy_db8_cD1  \\\n",
       "0     1.208096e-06     0.014643     0.000214           0.009321   \n",
       "1     3.874273e-06     0.012940     0.000167           0.006530   \n",
       "2    -1.273306e-05     0.015932     0.000254           0.009553   \n",
       "3     1.085941e-06     0.006558     0.000043           0.003244   \n",
       "4     1.026267e-05     0.020510     0.000421           0.009280   \n",
       "5     5.273178e-06     0.038599     0.001490           0.013889   \n",
       "6     4.526431e-06     0.020290     0.000412           0.007897   \n",
       "7     7.654016e-06     0.020083     0.000403           0.007329   \n",
       "8     8.145493e-06     0.017052     0.000291           0.010127   \n",
       "9     8.129573e-07     0.023415     0.000548           0.012639   \n",
       "10    0.000000e+00     0.010405     0.000108           0.006491   \n",
       "11    2.884911e-05     0.016135     0.000260           0.009331   \n",
       "12    0.000000e+00     0.004892     0.000024           0.002059   \n",
       "\n",
       "    skewness_db8_cD1  \n",
       "0           0.022624  \n",
       "1          -0.041386  \n",
       "2           0.060238  \n",
       "3           0.028574  \n",
       "4          -0.022495  \n",
       "5           0.074893  \n",
       "6           0.259115  \n",
       "7           0.488535  \n",
       "8          -0.033558  \n",
       "9          -0.008580  \n",
       "10          0.002405  \n",
       "11         -0.012464  \n",
       "12          0.003960  \n",
       "\n",
       "[13 rows x 219 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44100"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "librosa.core.get_samplerate('/Users/macbookretina/Desktop/mount-s3-bucket/fma_small/000/000002.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check size of  log-mel / constant-Q transform in gtzan\n",
    "\n",
    "# creating an empty list to store sizes in\n",
    "sizes = []\n",
    "\n",
    "print('extracting log-mel from gtzan dataset')\n",
    "\n",
    "for file in os.scandir(MOUNTED_DATASET_PATH + '/gtzan/wavfiles'):\n",
    "    if file.is_file():\n",
    "        # get sample rate of audio file\n",
    "        sample_rate = librosa.core.get_samplerate(file.path)\n",
    "\n",
    "        # load audio file as time series\n",
    "        time_series, _ = librosa.core.load(file.path, sample_rate)\n",
    "        \n",
    "        # compute cqt\n",
    "        cqt = librosa.cqt(time_series, sample_rate)\n",
    "        \n",
    "        # convert from amplitude to decibels unit\n",
    "        scaled_cqt = librosa.amplitude_to_db(cqt, ref=np.max) \n",
    "        \n",
    "        # adding the size to the list\n",
    "        sizes.append(scaled_cqt.shape)\n",
    "    \n",
    "# checking if all sizes are the same\n",
    "print(f'The sizes of all the log-mel in our data set are equal: {len(set(sizes)) == 1}')\n",
    "\n",
    "# checking the max size\n",
    "print(f'The maximum size is: {max(sizes)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check size of mel-spectogram in gtzan\n",
    "\n",
    "# creating an empty list to store sizes in\n",
    "sizes = []\n",
    "\n",
    "print('extracting mel-spectogram from gtzan dataset')\n",
    "\n",
    "for file in os.scandir(MOUNTED_DATASET_PATH + '/gtzan/wavfiles'):\n",
    "    if file.is_file():\n",
    "        # get sample rate of audio file\n",
    "        sample_rate = librosa.core.get_samplerate(file.path)\n",
    "\n",
    "        # load audio file as time series\n",
    "        time_series, _ = librosa.core.load(file.path, sample_rate)\n",
    "        \n",
    "        # compute spectogram\n",
    "        mel_spect = librosa.feature.melspectrogram(time_series, sample_rate)\n",
    "        \n",
    "        # convert spectogram to decibels unit \n",
    "        scaled_mel_spect = librosa.power_to_db(mel_spect, ref=np.max)\n",
    "        \n",
    "        # adding the size to the list\n",
    "        sizes.append(scaled_mel_spect.shape)\n",
    "    \n",
    "# checking if all sizes are the same\n",
    "print(f'The sizes of all the mel spectrograms in our data set are equal: {len(set(sizes)) == 1}')\n",
    "\n",
    "# checking the max size\n",
    "print(f'The maximum size is: {max(sizes)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check size of  log-mel / constant-Q transform in fma\n",
    "\n",
    "# creating an empty list to store sizes in\n",
    "sizes = []\n",
    "\n",
    "print('extracting log-mel from fma dataset')\n",
    "\n",
    "for directory in os.scandir(MOUNTED_DATASET_PATH + '/fma_small'):\n",
    "    if directory.is_dir():\n",
    "        for file in os.scandir(directory):\n",
    "            if file.is_file():\n",
    "                # get sample rate of audio file\n",
    "                sample_rate = librosa.core.get_samplerate(file.path)\n",
    "\n",
    "                # load audio file as time series\n",
    "                time_series, _ = librosa.core.load(file.path, sample_rate)\n",
    "\n",
    "                # compute cqt\n",
    "                cqt = librosa.cqt(time_series, sample_rate)\n",
    "\n",
    "                # convert from amplitude to decibels unit\n",
    "                scaled_cqt = librosa.amplitude_to_db(cqt, ref=np.max) \n",
    "\n",
    "                # adding the size to the list\n",
    "                sizes.append(scaled_cqt.shape)\n",
    "    \n",
    "# checking if all sizes are the same\n",
    "print(f'The sizes of all the log-mel in our data set are equal: {len(set(sizes)) == 1}')\n",
    "\n",
    "# checking the max size\n",
    "print(f'The maximum size is: {max(sizes)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check size of  mel-spectogram  in fma\n",
    "\n",
    "# creating an empty list to store sizes in\n",
    "sizes = []\n",
    "\n",
    "print('extracting mel-spectogram  from fma dataset')\n",
    "\n",
    "for directory in os.scandir(MOUNTED_DATASET_PATH + '/fma_small'):\n",
    "    if directory.is_dir():\n",
    "        for file in os.scandir(directory):\n",
    "            if file.is_file():\n",
    "                # get sample rate of audio file\n",
    "                sample_rate = librosa.core.get_samplerate(file.path)\n",
    "\n",
    "                # load audio file as time series\n",
    "                time_series, _ = librosa.core.load(file.path, sample_rate)\n",
    "\n",
    "                # compute spectogram\n",
    "                mel_spect = librosa.feature.melspectrogram(time_series, sample_rate)\n",
    "\n",
    "                # convert spectogram to decibels unit \n",
    "                scaled_mel_spect = librosa.power_to_db(mel_spect, ref=np.max)\n",
    "\n",
    "                # adding the size to the list\n",
    "                sizes.append(scaled_mel_spect.shape)\n",
    "    \n",
    "# checking if all sizes are the same\n",
    "print(f'The sizes of all the log-mel in our data set are equal: {len(set(sizes)) == 1}')\n",
    "\n",
    "# checking the max size\n",
    "print(f'The maximum size is: {max(sizes)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
